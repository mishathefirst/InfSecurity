{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используемые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dmitriair/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/dmitriair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных модуля nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Загрузка тренировочного датасета (файл должен быть в одной папке с исполняемым скриптом)\n",
    "df_train = pd.read_csv('Task_3_id_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование признаков\n",
    "\n",
    "## Стилистические признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все тексты в одной строке\n",
    "corpus = ' '.join(df_train.iloc[:, 0])\n",
    "\n",
    "# Вся используемая в корпусе пунктуация, сохраняется для дальнейшего использования\n",
    "punct = set([x for x in corpus if not x.isalnum() and not x.isspace()])\n",
    "\n",
    "# Распределение частот слов\n",
    "word_freq = Counter([stemmer.stem(x) for x in tokenizer.tokenize(corpus)])\n",
    "\n",
    "# Единожды использованные в корпусе слова, сохраняются для дальнейшего использования\n",
    "hapax_legomena = [x for x in word_freq.keys() if word_freq[x] == 1]\n",
    "\n",
    "# Дважды использованные в корпусе слова, сохраняются для дальнейшего использования\n",
    "dis_legomena = [x for x in word_freq.keys() if word_freq[x] == 2]\n",
    "\n",
    "\n",
    "def stylistic(df_column, hapax_legomena, dis_legomena, punct):\n",
    "    # Вектор имен признаков\n",
    "    stylistic_names = ['avg_word_len', 'short_word_freq', 'digit_freq', 'uppercase_freq', 'alpha_freq'] + \\\n",
    "                      ['hapax' + x for x in hapax_legomena] + ['dis' + x for x in dis_legomena] + \\\n",
    "                      ['stopword_freq'] + [x + '_freq' for x in punct]\n",
    "\n",
    "    stylistic_feature_array = []\n",
    "\n",
    "    for idx, text in tqdm(enumerate(df_column)):\n",
    "        words = tokenizer.tokenize(text)\n",
    "\n",
    "        # Средняя длина слов в тексте\n",
    "        avg_word_len = sum(len(x) for x in words) / len(words)\n",
    "\n",
    "        # Частота коротких слов\n",
    "        short_word_freq = len([x for x in words if len(x) <= 5]) / len(words)\n",
    "\n",
    "        # Частота цифр в тексте\n",
    "        digit_freq = sum(c.isdigit() for c in text) / len(text)\n",
    "\n",
    "        # Частота заглавных букв в тексте\n",
    "        uppercase_freq = sum(c.isupper() for c in text) / len(text)\n",
    "\n",
    "        # Частота букв в тексте\n",
    "        alpha_freq = sum(c.isalpha() for c in text) / len(text)\n",
    "\n",
    "        # Бинарный вектор для гапаксов корпуса\n",
    "        hapaxes = [x in words for x in hapax_legomena]\n",
    "\n",
    "        # Бинарный вектор для дислегоменонов корпуса\n",
    "        dises = [x in words for x in dis_legomena]\n",
    "\n",
    "        # Частота стоп-слов в тексте\n",
    "        stopword_freq = sum(x in stopwords.words('english') for x in words)\n",
    "\n",
    "        # Вектор частот знаков препинания в тексте\n",
    "        char_count = [text.count(x) / len(text) for x in punct]\n",
    "\n",
    "        stylistic_feature_array.append([avg_word_len, short_word_freq, digit_freq, uppercase_freq, alpha_freq]\n",
    "                                       + hapaxes + dises + [stopword_freq] + char_count)\n",
    "\n",
    "    stylistic_features = pd.DataFrame(stylistic_feature_array)\n",
    "    stylistic_features.columns = stylistic_names\n",
    "\n",
    "    return stylistic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержательные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [12:43<00:00, 254.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Векторизаторы n-грамм слов, сохраняются для дальнейшего использования\n",
    "word_vectorizers = {}\n",
    "\n",
    "# Предобработка текстов: удаление всего, кроме слов, и их стемминг\n",
    "def preprocess(text):\n",
    "    return ' '.join([stemmer.stem(x).lower() for x in tokenizer.tokenize(text) \\\n",
    "                     if x not in stopwords.words('english')])\n",
    "\n",
    "for i in tqdm(range(1, 4)):\n",
    "    vectorizer = CountVectorizer(ngram_range=(i, i), max_features=100, preprocessor=preprocess)\n",
    "    vectorizer.fit(df_train.iloc[:, 0].to_list())\n",
    "    word_vectorizers.update({i: vectorizer})\n",
    "\n",
    "def content(df_column, vectorizer_dict):\n",
    "    feature_names = []\n",
    "    content_features = pd.DataFrame()\n",
    "\n",
    "    for i in vectorizer_dict.keys():\n",
    "        vectorizer = vectorizer_dict[i]\n",
    "        ngram_counts = vectorizer.transform(df_column.to_list())\n",
    "\n",
    "        feature_names += ['_'.join(x.split(' ')) for x in vectorizer.get_feature_names()]\n",
    "        content_features = pd.concat([content_features, pd.DataFrame.sparse.from_spmatrix(ngram_counts)], axis=1)\n",
    "\n",
    "    content_features.columns = feature_names\n",
    "\n",
    "    return content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гибридные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Векторизаторы n-грамм символов, сохраняются для дальнейшего использования\n",
    "char_vectorizers = {}\n",
    "\n",
    "for i in tqdm(range(2, 4)):\n",
    "    vectorizer = CountVectorizer(ngram_range=(i, i), max_features=100, analyzer='char')\n",
    "    vectorizer.fit(df_train.iloc[:, 0].to_list())\n",
    "    char_vectorizers.update({i: vectorizer})\n",
    "\n",
    "def hybrid(df_column, vectorizer_dict):\n",
    "    feature_names = []\n",
    "    hybrid_features = pd.DataFrame()\n",
    "\n",
    "    for i in vectorizer_dict.keys():\n",
    "        vectorizer = vectorizer_dict[i]\n",
    "        ngram_counts = vectorizer.transform(df_column.to_list())\n",
    "\n",
    "        feature_names += vectorizer.get_feature_names()\n",
    "        hybrid_features = pd.concat([hybrid_features, pd.DataFrame.sparse.from_spmatrix(ngram_counts)], axis=1)\n",
    "\n",
    "    hybrid_features.columns = feature_names\n",
    "\n",
    "    return hybrid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сокращение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylistic_train = stylistic(df_train.iloc[:, 0], hapax_legomena, dis_legomena, punct)\n",
    "\n",
    "content_train = content(df_train.iloc[:, 0], word_vectorizers)\n",
    "hybrid_train = hybrid(df_train.iloc[:, 0], char_vectorizers)\n",
    "\n",
    "# Матрица из всех сгенерированных признаков\n",
    "x_train = pd.concat([stylistic_train, content_train, hybrid_train], axis=1)\n",
    "y_train = df_train.iloc[:, 1]\n",
    "\n",
    "# Эстимейтор, использующийся для удаления признаков. Необходимо указать определенный random_state\n",
    "rfc = RandomForestClassifier(n_jobs=-3, random_state=1000)\n",
    "\n",
    "# Селектор признаков. Шаг указывается равным 0.1 для быстроты вычислений, минимальное количество остающихся\n",
    "# признаков равно 10% количества изначальных признаков (округляется)\n",
    "selector = RFECV(rfc, step=0.1, cv=5, min_features_to_select=round(x_train.shape[1] * 0.1))\n",
    "selector = selector.fit(x_train, y_train)\n",
    "\n",
    "feature_mask = selector.support_\n",
    "\n",
    "# В финальном датасете оставляются все признаки, выбранные селектором\n",
    "x_train = x_train.iloc[:, feature_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняем тренировочный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.concat([x_train, y_train], axis=1)\n",
    "a.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [06:58,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('Task_3_id_test.csv')\n",
    "\n",
    "# При формировании признаков используются данные, полученные с тренировочного датасета\n",
    "stylistic_test = stylistic(df_test.iloc[:, 0], hapax_legomena, dis_legomena, punct)\n",
    "content_test = content(df_test.iloc[:, 0], word_vectorizers)\n",
    "hybrid_test = hybrid(df_test.iloc[:, 0], char_vectorizers)\n",
    "\n",
    "x_test = pd.concat([stylistic_test, content_test, hybrid_test], axis=1)\n",
    "y_test = df_test.iloc[:, 1]\n",
    "\n",
    "# Оставляются те же признаки, что были выбраны для тренировочного датасета\n",
    "x_test = x_test.iloc[:, feature_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняем тестовый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.concat([x_test, y_test], axis=1)\n",
    "b.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимание, что в данном случае тренировочный и тестовый датасеты имеют по 2500 объектов. Это сделано для того, чтобы продемонстрировать, каким образом производить векторизацию тестовых объектов, чтобы тестовые объекты обладали тем же набором признаков. Для выполнения индивидуального задания эти обработанные датасеты объединялись в один."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
